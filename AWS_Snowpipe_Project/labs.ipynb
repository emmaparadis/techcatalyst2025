{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552bfda0",
   "metadata": {},
   "source": [
    "#### Analysis Table:\n",
    "\n",
    "| **Method**         | **Purpose** | **Best Use Cases** | **Pros** | **Cons** |\n",
    "| ------------------ | ----------- | ------------------ | -------- | -------- |\n",
    "| `upload_file`      | Upload file to an S3 bucket. | large-file parallel uploading | efficient handling of large files | less advanced configurations / no ACLs |\n",
    "| `upload_fileobj`   | Uploads a file-like object to S3 | file-like object uploading directly to S3 | managed transfer / can use ACLs | Can only be opened in binary mode not text mode |\n",
    "| `put_object`       | add an object to an S3 bucket | Uploading files with specific configurations | Can use ACLs / Can add metadata | No support for multipart uploads / file must be < 5gb  |\n",
    "| `download_file`    | downloads an object and saves to file | When you want to save an object locally | Support extra arguments and callback parameters  | does not support downloading full buckets         |\n",
    "| `download_fileobj` | downloads a file like object to a file | When you want to save a file-like object locally | can handle file like objects | Can only be opened in binary mode |\n",
    "| `get_object`       | retrieve an object from S3 | To view an object in a bucket using a key | faster than manual file handling | complex to implement and manage |\n",
    "\n",
    "#### Reflection Questions:\n",
    "\n",
    "1. **Upload Methods**:\n",
    "   - What are the key differences between `upload_file`, `upload_fileobj`, and `put_object`?\n",
    "\n",
    "    upload file is efficient at handling large files due to multipart uploads, upload_fileobj can only handle file-type ojects in binary and does not support large files < 5gb, put_object can upload files with specific configs but also struggles with larger files. \n",
    "   - When would you choose to use `put_object` over `upload_file` or `upload_fileobj`?\n",
    "   \n",
    "    I would choose put_object for when I want to add an object to S3 with specific configurations, I would use upload_file for files larger than 5gb, and I would use upload_filobj for files that needed to be read in binary format. \n",
    "2. **Download Methods**:\n",
    "   - How does `download_file` differ from `download_fileobj` and `get_object`?\n",
    "   - In what scenarios would `get_object` be more beneficial than `download_file`?\n",
    "3. **Efficiency and Performance**:\n",
    "   - How do multipart uploads and downloads enhance the performance of file transfer operations?\n",
    "   - What are the limitations of using `put_object` and `get_object` for large files?\n",
    "4. **Practical Applications**:\n",
    "   - Consider a scenario where you need to upload a large video file to S3. Which method would you use and why?\n",
    "   - If you need to process data in memory before saving it locally, which download method would be most suitable?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c53db",
   "metadata": {},
   "source": [
    "# Lab 2: Exploring AWS Boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630427c",
   "metadata": {},
   "source": [
    "### Load your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9a3a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c66d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_client = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d21510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capstone-techcatalyst-conformed\n",
      "capstone-techcatalyst-raw\n",
      "capstone-techcatalyst-transformed\n",
      "techcatalyst-public\n",
      "techcatalyst-raw\n",
      "techcatalyst-transformed\n"
     ]
    }
   ],
   "source": [
    "buckets = s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']:\n",
    "    if 'techcatalyst' in bucket['Name']:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f2c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE/test_export.parquet\n",
      "BLAKE/upload_file_method_GOOG.csv\n",
      "BLAKE/upload_fileobj_method.txt\n",
      "Ben/Million_Songs/\n",
      "Ben/bingchilling.txt\n",
      "Ben/gooooog.csv\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000000_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000001_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000002_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000003_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000004_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000005_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000006_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000007_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000008_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000009_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000010_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000011_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000012_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000013_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000014_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000015_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000016_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000017_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000018_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000019_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000020_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000021_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000022_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000023_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000024_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000025_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000026_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000027_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000028_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000029_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000030_000000.snappy.parquet\n",
      "Ben/parquetGoogleStock/2c8da4b4fca54ff9b2e97ab78360ace4_000031_000000.snappy.parquet\n",
      "Ben/uploads/Google_upload_test\n",
      "EMMA/test_export.parquet\n",
      "MELISSA/test_export.parquet\n",
      "MillionSongSubset/\n",
      "MillionSongSubset/log-data/2018-11-01-events.json\n",
      "MillionSongSubset/log-data/2018-11-02-events.json\n",
      "MillionSongSubset/log-data/2018-11-03-events.json\n",
      "MillionSongSubset/log-data/2018-11-04-events.json\n",
      "MillionSongSubset/log-data/2018-11-05-events.json\n",
      "MillionSongSubset/log-data/2018-11-06-events.json\n",
      "MillionSongSubset/log-data/2018-11-07-events.json\n",
      "MillionSongSubset/log-data/2018-11-08-events.json\n",
      "MillionSongSubset/log-data/2018-11-09-events.json\n",
      "MillionSongSubset/log-data/2018-11-10-events.json\n",
      "MillionSongSubset/log-data/2018-11-11-events.json\n",
      "MillionSongSubset/log-data/2018-11-12-events.json\n",
      "MillionSongSubset/log-data/2018-11-13-events.json\n",
      "MillionSongSubset/log-data/2018-11-14-events.json\n",
      "MillionSongSubset/log-data/2018-11-15-events.json\n",
      "MillionSongSubset/log-data/2018-11-16-events.json\n",
      "MillionSongSubset/log-data/2018-11-17-events.json\n",
      "MillionSongSubset/log-data/2018-11-18-events.json\n",
      "MillionSongSubset/log-data/2018-11-19-events.json\n",
      "MillionSongSubset/log-data/2018-11-20-events.json\n",
      "MillionSongSubset/log-data/2018-11-21-events.json\n",
      "MillionSongSubset/log-data/2018-11-22-events.json\n",
      "MillionSongSubset/log-data/2018-11-23-events.json\n",
      "MillionSongSubset/log-data/2018-11-24-events.json\n",
      "MillionSongSubset/log-data/2018-11-25-events.json\n",
      "MillionSongSubset/log-data/2018-11-26-events.json\n",
      "MillionSongSubset/log-data/2018-11-27-events.json\n",
      "MillionSongSubset/log-data/2018-11-28-events.json\n",
      "MillionSongSubset/log-data/2018-11-29-events.json\n",
      "MillionSongSubset/log-data/2018-11-30-events.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAAW128F429D538.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAABD128F429CF47.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAADZ128F9348C2E.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAEF128F4273421.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAFD128F92F423A.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAMO128F1481E7F.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAMQ128F1460CD3.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAPK128E0786D96.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAARJ128F9320760.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAVG12903CFA543.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAVO128F93133D4.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABCL128F4286650.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABDL12903CAABBA.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABJL12903CDCF1A.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABJV128F1460C49.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABLR128F423B7E3.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABNV128F425CEE1.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABRB128F9306DD5.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABVM128F92CA9DC.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABXG128F9318EBD.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABYN12903CFD305.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABYW128F4244559.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACCG128F92E8A55.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACER128F4290F96.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACFV128F935E50B.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACHN128F1489601.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACIW12903CC0F6D.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACLV128F427E123.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACNS128F14A2DF5.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACOW128F933E35F.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACPE128F421C1B9.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACQT128F9331780.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACSL128F93462F4.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACTB12903CAAF15.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACVS128E078BE39.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACZK128F4243829.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABACN128F425B784.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAFJ128F42AF24E.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAFP128F931E9A1.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAIO128F42938F9.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABATO128F42627E9.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAVQ12903CBF7E0.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAWW128F4250A31.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXL128F424FC50.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXR128F426515F.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXV128F92F6AE3.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAZH128F930419A.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBAM128F429D223.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBBV128F42967D7.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBJE12903CDB442.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBKX128F4285205.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBLU128F93349CF.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBNP128F932546F.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBOP128F931B50D.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBOR128F4286200.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBTA128F933D304.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBVJ128F92F7EAA.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBXU128F92FEF48.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBZN12903CD9297.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCAJ12903CDFCC2.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCEC128F426456E.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCEI128F424C983.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCFL128F149BB0D.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCIX128F4265903.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCKL128F423A778.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCPZ128F4275C32.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCRU128F423F449.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCTK128F934B224.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCUQ128E0783E2B.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCXB128F4286BD3.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCYE128F934CE1D.json\n",
      "YOURNAME/test_export.parquet\n",
      "accidents/\n",
      "accidents/accidents_2017_to_2023_english.csv\n",
      "alexia/test_export.parquet\n",
      "emma/c20bdb605ad448f1ba9ced9f9f4ea51a_000000_000000.snappy.parquet\n",
      "emma/c20bdb605ad448f1ba9ced9f9f4ea51a_000001_000000.snappy.parquet\n",
      "emma/c20bdb605ad448f1ba9ced9f9f4ea51a_000002_000000.snappy.parquet\n",
      "emma/c20bdb605ad448f1ba9ced9f9f4ea51a_000003_000000.snappy.parquet\n",
      "fabiola/fabiola_GOOG.csv\n",
      "fabiola/test_export.parquet\n",
      "jaden/cbad0a037ce54ad9a54917303dfdfe53.snappy.parquet\n",
      "michael/e798e42882254cf5a9e99351bd7626d6.snappy.parquet\n",
      "miraj/test_export.parquet\n",
      "shaswat/\n",
      "stage/\n",
      "stage/yellow_tripdata.csv\n",
      "stage/yellow_tripdata.json\n",
      "stage/yellow_tripdata.parquet\n",
      "stocks/\n",
      "stocks/GOOG.csv\n",
      "suchitha/test_export.parquet\n",
      "tatwan/51addc9167684aa281c96fd5ae2d5be2.snappy.parquet\n",
      "tatwan/GOOG.csv\n",
      "tatwan/GOOG_NEW.csv\n",
      "tatwan/buffer.txt\n",
      "tyler/test_export.parquet\n",
      "yellow_tripdata_2024-01.parquet\n",
      "yellow_tripdata_2024-02.parquet\n",
      "yellow_tripdata_2024-03.parquet\n",
      "yellow_tripdata_2024-04.parquet\n"
     ]
    }
   ],
   "source": [
    "# list objects in a specific bucket \"techcatalyst-raw\" \n",
    "bucket_name = 'techcatalyst-raw'\n",
    "objects = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "for obj in objects.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec3d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE/upload_file_method_GOOG.csv\n",
      "Ben/gooooog.csv\n",
      "accidents/accidents_2017_to_2023_english.csv\n",
      "fabiola/fabiola_GOOG.csv\n",
      "stage/yellow_tripdata.csv\n",
      "stocks/GOOG.csv\n",
      "tatwan/GOOG.csv\n",
      "tatwan/GOOG_NEW.csv\n"
     ]
    }
   ],
   "source": [
    "# list objects that are CSV in a specific bucket \"techcatalyst-raw\" \n",
    "for obj in objects.get('Contents', []):\n",
    "    if (obj['Key']).endswith('.csv'):\n",
    "        print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607fe58",
   "metadata": {},
   "source": [
    "### Downloading an objects using `download_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9951de",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(Bucket='techcatalyst-raw',  # from which bucket\n",
    "                        Key='stocks/GOOG.csv',\n",
    "                        Filename='emma_goog.csv') # Filename is what you want to call it once it is downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d981a62",
   "metadata": {},
   "source": [
    "### Downloading an objects using `download_fileobj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1375c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "io_temp = io.BytesIO()\n",
    "temp = s3_client.download_fileobj(Bucket='techcatalyst-raw',  # from which bucket\n",
    "                                Key='stocks/GOOG.csv',\n",
    "                                Fileobj=io_temp) # pass th io.BytesIO object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ade7197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Date,Open,High,Low,Close,Volume\\r\\n1/2/2025 16:00:00,191.49,193.2,188.71,190.63,17545162\\r\\n1/3/2025 16:'\n"
     ]
    }
   ],
   "source": [
    "print(io_temp.getvalue()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b89acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Date,Open,High,Low,Close,Volume\\r\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(io_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6092e90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io_temp.seek(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ff8d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('google_stock_downloaded.csv', 'wb') as f:\n",
    "    f.write(io_temp.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffef33",
   "metadata": {},
   "source": [
    "### Uploading a local file using `upload_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac39a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='emma_goog.csv', # local file name\n",
    "                      Bucket='techcatalyst-raw', # the bucket target\n",
    "                      Key='EMMA/emna_goog.csv') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b9f21",
   "metadata": {},
   "source": [
    "### Uploading using `upload_fileobj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4338d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_file = io.BytesIO(b\"Uploading in memeory file!\")\n",
    "s3_client.upload_fileobj(Fileobj=io_temp,\n",
    "                          Bucket='techcatalyst-raw', \n",
    "                          Key='EMMA/emna_goog.txt') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ef853",
   "metadata": {},
   "source": [
    "### List all the objects in your bucket/username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10d5842b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'EMMA/emna_goog.csv',\n",
       "  'LastModified': datetime.datetime(2025, 8, 4, 19, 43, 46, tzinfo=tzlocal()),\n",
       "  'ETag': '\"8cbbdc687ee45f1fe58a522e16d423c2\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 7889,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'EMMA/emna_goog.txt',\n",
       "  'LastModified': datetime.datetime(2025, 8, 4, 19, 46, 32, tzinfo=tzlocal()),\n",
       "  'ETag': '\"5dedeaccf30c891106b85b31cfa718ea\"',\n",
       "  'ChecksumAlgorithm': ['CRC32'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 7856,\n",
       "  'StorageClass': 'STANDARD'},\n",
       " {'Key': 'EMMA/test_export.parquet',\n",
       "  'LastModified': datetime.datetime(2025, 7, 25, 15, 23, 16, tzinfo=tzlocal()),\n",
       "  'ETag': '\"3c8cff84500ad4c4c85361e38dce05ed-1\"',\n",
       "  'ChecksumAlgorithm': ['CRC64NVME'],\n",
       "  'ChecksumType': 'FULL_OBJECT',\n",
       "  'Size': 8393,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects = s3_client.list_objects_v2(Bucket='techcatalyst-raw', Prefix='EMMA/')\n",
    "objects.keys()\n",
    "objects['Contents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb2aa6",
   "metadata": {},
   "source": [
    "# Lab 3: Introduction to AWS Wrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd1fcfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awswrangler in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.12.1)\n",
      "Requirement already satisfied: boto3<2,>=1.20.32 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from awswrangler) (1.39.15)\n",
      "Requirement already satisfied: botocore<2,>=1.23.32 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from awswrangler) (1.39.15)\n",
      "Requirement already satisfied: numpy<3.0,>=1.26 in /home/codespace/.local/lib/python3.12/site-packages (from awswrangler) (2.3.1)\n",
      "Requirement already satisfied: packaging<26.0,>=21.1 in /home/codespace/.local/lib/python3.12/site-packages (from awswrangler) (25.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from awswrangler) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<21.0.0,>=18.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from awswrangler) (18.1.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from awswrangler) (80.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from awswrangler) (4.14.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3<2,>=1.20.32->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3<2,>=1.20.32->awswrangler) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<2,>=1.23.32->awswrangler) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<2,>=1.23.32->awswrangler) (2.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2,>=1.23.32->awswrangler) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e9377d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fa875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ea5df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 14:19:55,303\tWARNING services.py:2142 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.98gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-08-05 14:19:56,690\tINFO worker.py:1927 -- Started a local Ray instance.\n",
      "2025-08-05 14:19:59,869\tWARNING file_meta_provider.py:202 -- Skipping expansion of 1 path(s). If your paths contain directories or if file size collection is required, try rerunning this read with `meta_provider=DefaultFileMetadataProvider()`.\n",
      "2025-08-05 14:20:02,447\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_1_0\n",
      "2025-08-05 14:20:02,496\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_1_0. Full logs are in /tmp/ray/session_2025-08-05_14-19-46_902452_2542/logs/ray-data\n",
      "2025-08-05 14:20:02,498\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_1_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadArrowCSV] -> AggregateNumRows[AggregateNumRows]\n",
      "2025-08-05 14:20:02,519\tWARNING resource_manager.py:130 -- ⚠️  Ray's object store is configured to use only 42.9% of available memory (1.9GB out of 4.5GB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 14:20:04,644\tINFO streaming_executor.py:231 -- ✔️  Dataset dataset_1_0 execution finished in 2.15 seconds\n",
      "2025-08-05 14:20:04,689\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_0_0\n",
      "2025-08-05 14:20:04,707\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_0_0. Full logs are in /tmp/ray/session_2025-08-05_14-19-46_902452_2542/logs/ray-data\n",
      "2025-08-05 14:20:04,710\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_0_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadArrowCSV]\n",
      "2025-08-05 14:20:05,330\tINFO streaming_executor.py:231 -- ✔️  Dataset dataset_0_0 execution finished in 0.62 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date    Open    High     Low   Close    Volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wr.s3.read_csv('s3://techcatalyst-raw/stocks/GOOG.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69c7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'modin.pandas.dataframe.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    140 non-null    object \n",
      " 1   Open    140 non-null    float64\n",
      " 2   High    140 non-null    float64\n",
      " 3   Low     140 non-null    float64\n",
      " 4   Close   140 non-null    float64\n",
      " 5   Volume  140 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8102700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Database       Description\n",
      "0           alexia_logs                  \n",
      "1           alexia_song                  \n",
      "2      awswrangler_test                  \n",
      "3                ben_db                  \n",
      "4              ben_song                  \n",
      "5              ben_taxi                  \n",
      "6            blake_taxi                  \n",
      "7           blake_wr_db                  \n",
      "8               default  default database\n",
      "9               emma_db                  \n",
      "10           fabiola_db                  \n",
      "11           jaden_taxi                  \n",
      "12        jadenastle_db                  \n",
      "13         melissa_logs                  \n",
      "14        melissa_songs                  \n",
      "15           michael_db                  \n",
      "16                my_db                  \n",
      "17           shaswat_db                  \n",
      "18         shaswat_logs                  \n",
      "19         shaswat_song                  \n",
      "20          suchitha_db                  \n",
      "21            tatwan_db                  \n",
      "22  tatwan_inclass_demo                  \n",
      "23          tatwan_taxi                  \n"
     ]
    }
   ],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14bb21b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma_db</td>\n",
       "      <td>emma_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Database       Table Description       TableType  \\\n",
       "0  emma_db  emma_stock              EXTERNAL_TABLE   \n",
       "\n",
       "                                Columns Partitions  \n",
       "0  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'emma'\n",
    "database_name = f\"{name}_db\"\n",
    "wr.catalog.tables(database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33078e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 20:03:50,256\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_2_0\n",
      "2025-08-04 20:03:50,879\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_3_0\n",
      "2025-08-04 20:03:51,836\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_6_0\n",
      "2025-08-04 20:03:51,840\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_6_0. Full logs are in /tmp/ray/session_2025-08-04_20-00-04_029478_48442/logs/ray-data\n",
      "2025-08-04 20:03:51,842\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_6_0: InputDataBuffer[Input] -> TaskPoolMapOperator[Write]\n",
      "2025-08-04 20:03:53,038\tINFO streaming_executor.py:231 -- ✔️  Dataset dataset_6_0 execution finished in 1.20 seconds\n",
      "2025-08-04 20:03:53,062\tINFO dataset.py:4619 -- Data sink ArrowParquet finished. 140 rows and 15.1KB data written.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/emma/cdfabb0fcbff483cb6906ce39afc203a_000000_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/emma/cdfabb0fcbff483cb6906ce39afc203a_000001_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/emma/cdfabb0fcbff483cb6906ce39afc203a_000002_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/emma/cdfabb0fcbff483cb6906ce39afc203a_000003_000000.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df, # the DataFrame you just created \n",
    "    path=f\"s3://techcatalyst-raw/{name}/\", # write to the techcatalyst-raw bucket under your folder name (or it would create a new folder if it does not exist)\n",
    "    dataset=True, \n",
    "    database='emma_db', # the name of the database you just created in AWS Glue \n",
    "    table= 'emma_stock', # pick a table name for example YOURNAME_STOCK\n",
    "    mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7d888ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben_db</td>\n",
       "      <td>ben_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blake_wr_db</td>\n",
       "      <td>blake_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emma_db</td>\n",
       "      <td>emma_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fabiola_db</td>\n",
       "      <td>fabiola_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jadenastle_db</td>\n",
       "      <td>jaden_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>michael_db</td>\n",
       "      <td>michael_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shaswat_db</td>\n",
       "      <td>shaswat_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>suchitha_db</td>\n",
       "      <td>suchitha_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td>tatwan_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tatwan_inclass_demo</td>\n",
       "      <td>tatwan_goog_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Database              Table              Description  \\\n",
       "0               ben_db          ben_stock  This is my stock table.   \n",
       "1          blake_wr_db        blake_stock  This is my stock table.   \n",
       "2              emma_db         emma_stock                            \n",
       "3           fabiola_db      fabiola_stock  This is my stock table.   \n",
       "4        jadenastle_db        jaden_stock  This is my stock table.   \n",
       "5           michael_db      michael_stock                            \n",
       "6           shaswat_db      shaswat_stock  This is my stock table.   \n",
       "7          suchitha_db     suchitha_stock  This is my stock table.   \n",
       "8            tatwan_db       tatwan_stock  This is my stock table.   \n",
       "9  tatwan_inclass_demo  tatwan_goog_stock                            \n",
       "\n",
       "        TableType                               Columns Partitions  \n",
       "0  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "1  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "2  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "3  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "4  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "5  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "6  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "7  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "8  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "9  EXTERNAL_TABLE  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(name_contains=\"stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "df = wr.s3.read_parquet_table(database='emma_db',\n",
    "                               table='emma_stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "444193e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 'string',\n",
       " 'open': 'double',\n",
       " 'high': 'double',\n",
       " 'low': 'double',\n",
       " 'close': 'double',\n",
       " 'volume': 'bigint'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.get_table_types(database='emma_db',\n",
    "                               table='emma_stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "879bad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_details = wr.catalog.get_tables(database='emma_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfb4bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"This is my stock table.\"\n",
    "param = {\"source\": \"Google\", \"class\": \"stock\"}\n",
    "comments = {\n",
    "    \"Date\": \"Trading Date\",\n",
    "    \"Open\": \"Opening Price\",\n",
    "    \"Close\": \"Closing Price\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31e5bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 20:13:10,330\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_8_0\n",
      "2025-08-04 20:13:10,461\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_9_0\n",
      "2025-08-04 20:13:11,413\tWARNING plan.py:469 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/latest/data/data-internals.html#ray-data-and-tune\n",
      "2025-08-04 20:13:11,414\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_12_0\n",
      "2025-08-04 20:13:11,418\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_12_0. Full logs are in /tmp/ray/session_2025-08-04_20-00-04_029478_48442/logs/ray-data\n",
      "2025-08-04 20:13:11,422\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_12_0: InputDataBuffer[Input] -> TaskPoolMapOperator[Write]\n",
      "2025-08-04 20:13:12,617\tINFO streaming_executor.py:231 -- ✔️  Dataset dataset_12_0 execution finished in 1.20 seconds\n",
      "2025-08-04 20:13:12,629\tINFO dataset.py:4619 -- Data sink ArrowParquet finished. 140 rows and 15.1KB data written.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000001_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000000_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000002_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000003_000000.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path='s3://techcatalyst-raw/emma/',\n",
    "    dataset=True,\n",
    "    database='emma_db',\n",
    "    table='emma_stock',\n",
    "    mode='overwrite',\n",
    "    glue_table_settings=wr.typing.GlueTableSettings(description=desc,  # here we are passing some metadata\n",
    "                                                    parameters=param, \n",
    "                                                    columns_comments=comments),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45887bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Partition</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>string</td>\n",
       "      <td>False</td>\n",
       "      <td>Trading Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>Opening Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>close</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>Closing Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volume</td>\n",
       "      <td>bigint</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column Name    Type  Partition        Comment\n",
       "0        date  string      False   Trading Date\n",
       "1        open  double      False  Opening Price\n",
       "2        high  double      False               \n",
       "3         low  double      False               \n",
       "4       close  double      False  Closing Price\n",
       "5      volume  bigint      False               "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.table(database='emma_db', table='emma_stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dbdad9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000000_000000.snappy.parquet',\n",
       " 's3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000001_000000.snappy.parquet',\n",
       " 's3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000002_000000.snappy.parquet',\n",
       " 's3://techcatalyst-raw/emma/3a1b7394faa84991a5e7700c963af73f_000003_000000.snappy.parquet']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects('s3://techcatalyst-raw/emma/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "470ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.download(path='s3://techcatalyst-raw/stocks/GOOG.csv', \n",
    "               local_file='./new_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2935cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'emma'\n",
    "file_name = 'wr_emma_stock'\n",
    "wr.s3.upload(local_file='new_file.csv',path= f's3://techcatalyst-raw/{your_name}/uploads/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca0bef7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/emma/uploads/wr_emma_stock']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects(f's3://techcatalyst-raw/{your_name}/uploads/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2db209",
   "metadata": {},
   "source": [
    "### Exercise (using Glue Catalog and Athena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31436c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'emma_taxi'\n",
    "table_name = 'emma_tripdata'\n",
    "s3_path_directory = 's3://techcatalyst-raw/'\n",
    "s3_path_file = 's3://techcatalyst-raw/taxi_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "836f0cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.delete_table_if_exists(database=db_name, table=table_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "731a4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wr.catalog.create_database(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cc2e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 20:23:45,457\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_14_0\n",
      "2025-08-04 20:23:45,544\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_15_0\n",
      "2025-08-04 20:23:46,135\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_18_0\n",
      "2025-08-04 20:23:46,137\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_18_0. Full logs are in /tmp/ray/session_2025-08-04_20-00-04_029478_48442/logs/ray-data\n",
      "2025-08-04 20:23:46,138\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_18_0: InputDataBuffer[Input] -> TaskPoolMapOperator[Write]\n",
      "2025-08-04 20:23:46,887\tINFO streaming_executor.py:231 -- ✔️  Dataset dataset_18_0 execution finished in 0.75 seconds\n",
      "2025-08-04 20:23:46,901\tINFO dataset.py:4619 -- Data sink ArrowParquet finished. 140 rows and 15.1KB data written.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/taxi_data/8d12668ed190477fa03a29a0a5d50e84_000001_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/taxi_data/8d12668ed190477fa03a29a0a5d50e84_000000_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/taxi_data/8d12668ed190477fa03a29a0a5d50e84_000002_000000.snappy.parquet',\n",
       "  's3://techcatalyst-raw/taxi_data/8d12668ed190477fa03a29a0a5d50e84_000003_000000.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df, # the DataFrame you just created \n",
    "    path=f\"{s3_path_directory}{s3_path_file}\", # write to the techcatalyst-raw bucket under your folder name (or it would create a new folder if it does not exist)\n",
    "    dataset=True, \n",
    "    database=db_name, # the name of the database you just created in AWS Glue \n",
    "    table=table_name, # pick a table name for example YOURNAME_STOCK\n",
    "    mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65b79645",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_types, partitions_types = wr.s3.read_parquet_metadata(path=s3_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc4d9444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'emma_tripdata' created successfully in database 'emma_taxi'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wr.catalog.create_parquet_table(\n",
    "    database=db_name, # pass the database name\n",
    "    table=table_name, # pass the table name\n",
    "    path='s3://techcatalyst-raw/emma', # use the directoy here \n",
    "    columns_types=columns_types,  # Pass the schema here\n",
    "    partitions_types=partitions_types\n",
    ")\n",
    "print(f\"Table '{table_name}' created successfully in database '{db_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d780e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
    "\n",
    "df = wr.athena.read_sql_query(query, database=db_name)\n",
    "\n",
    "print(\"\\nQuery Results:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbac204",
   "metadata": {},
   "source": [
    "### Lambda Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde5eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='test.csv', # local file name\n",
    "                      Bucket='techcatalyst-public', # the bucket target\n",
    "                      Key='emma/test.csv') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5840a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.upload(local_file='test.csv',path= f's3://techcatalyst-public/emma/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fc0c3",
   "metadata": {},
   "source": [
    "### Automating Snowpipe for Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0be1eb",
   "metadata": {},
   "source": [
    "#### Made a Snowpipe that ingests a parquet file into the stage in snowflake. Used am ARN to connect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1fe9c",
   "metadata": {},
   "source": [
    "## Final\n",
    "\n",
    "#### Bringing it all together\n",
    "\n",
    "* End to end solution diagram \n",
    "\n",
    "!architecturediagram.png\n",
    "\n",
    "* Use cases for this solution?\n",
    "    * This pipeline could be used to ingest raw csv data into AWS for storage (Datalake), then the transformed parquet file can be ingested into snowflake for analysis (ETL Pipeline)\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
